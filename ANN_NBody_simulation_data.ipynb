{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5685f17d",
   "metadata": {},
   "source": [
    "# This notebook will implement Deep learning techniques for a particle-in-halo classification framework. Refer to the original publicly notebook in [ChJazhiel notebook](https://github.com/ChJazhiel/ML_ICF/blob/master/RF_Particles_z23.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47366a49",
   "metadata": {},
   "source": [
    "## For starters, we need to import the libraries and some other utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d76f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463edce",
   "metadata": {},
   "source": [
    "## Import our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8611873",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = np.load('/home/jazhiel/ML_Notebooks/Cosmology_ML/OUTFILE1M.npz') #np.load('/path/to/nbody/outfile.npz')\n",
    "test_flags  = data_dict['test_flags'] ## not important\n",
    "test_hosts  = data_dict['test_hosts'] ### somewhat relevant\n",
    "test_mass   = data_dict['test_mass'] ## important\n",
    "test_labels = data_dict['test_labels'] ## important\n",
    "test_input  = data_dict['test_input'] ## very important\n",
    "#test_snid   = dict_data['test_snid']\n",
    "#test_labels = dict_data['test_labels']\n",
    "print(test_mass) ## Here I want to check how is the halo mass matrix composed of, the -1 means the halo \n",
    "#is not in our range of 10^12-10^13 M_sun\n",
    "print(np.sum(test_labels)) ## Here I want to check how many label \"1\" do we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f585b",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5683e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### here we create or 10 vector dataset, I wonder if by adding some other information the classification will be better\n",
    "### adding mass is not helpful, the classifier is perfect in that regard\n",
    "dr1 = pd.DataFrame(test_input[0], columns = ['dr1'])\n",
    "dr2 = pd.DataFrame(test_input[1], columns = ['dr2'])\n",
    "dr3 = pd.DataFrame(test_input[2], columns = ['dr3'])\n",
    "dr4 = pd.DataFrame(test_input[3], columns = ['dr4'])\n",
    "dr5 = pd.DataFrame(test_input[4], columns = ['dr5'])\n",
    "dr6 = pd.DataFrame(test_input[5], columns = ['dr6'])\n",
    "dr7 = pd.DataFrame(test_input[6], columns = ['dr7'])\n",
    "dr8 = pd.DataFrame(test_input[7], columns = ['dr8'])\n",
    "dr9 = pd.DataFrame(test_input[8], columns = ['dr9'])\n",
    "dr10 = pd.DataFrame(test_input[9], columns = ['dr10'])\n",
    "#mass = pd.DataFrame(test_mass, columns = ['Halo_Mass'])\n",
    "lbl = pd.DataFrame(test_labels, columns =['labels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b35048",
   "metadata": {},
   "source": [
    "## Select all features and create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ae1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([dr1, dr2, dr3, dr4, dr5, dr6, dr7, dr8, dr9, dr10, lbl], axis=1, ignore_index=False, sort=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebe86a",
   "metadata": {},
   "source": [
    "## Here we select a dataframe consisting of evenly separated labels\n",
    "\n",
    "(I'm not sure if selecting all particles will impact in a different result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d16555",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sorting out the labels consisting in label '0' and label '1'\n",
    "## Then we sample them in order to not selecting them in a specific range or shape\n",
    "\n",
    "df_0 = df.sort_values('labels').head(710036).sample(289000)\n",
    "df_1 = df.sort_values('labels').tail(289964).sample(289000) \n",
    "df_1.labels.sum()\n",
    "df_r = pd.concat([df_0, df_1])\n",
    "#df_scaled = scaler.fit_transform(df_r.drop(['labels'], axis = 1))  \n",
    "#df_copy1 = pd.DataFrame(df_scaled, columns=['dr1', 'dr2', 'dr3', 'dr4', 'dr5', 'dr6', 'dr7', 'dr8', 'dr9', 'dr10'])\n",
    "\n",
    "shuffle_df = df_r.sample(frac = 1.0)\n",
    "#shuffle_df.insert(10, \"Model\", ['LC','SF']*14300, True)\n",
    "shuffle_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6945ae",
   "metadata": {},
   "source": [
    "## define a size for our traning dataset \n",
    "\n",
    "I think that for 500k + particles we can divide into train, test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a size for your train set \n",
    "train_size = int(0.8 * len(df_r))\n",
    "\n",
    "# Split your dataset \n",
    "train_set = shuffle_df[:train_size]\n",
    "test_set = shuffle_df[train_size:]\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37a45e",
   "metadata": {},
   "source": [
    "## Select data, X for attributes, y for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shuffle_df.drop(['labels'], axis = 1)\n",
    "ylabels = shuffle_df.labels\n",
    "#ymodel = shuffle_df.Model\n",
    "#ymodel = copy1.Model\n",
    "X_train = train_set.drop(['labels'], axis = 1)\n",
    "X_test = test_set.drop(['labels'], axis= 1)\n",
    "y_trainl = train_set.labels\n",
    "y_testl = test_set.labels\n",
    "#y_trainM = train_set.Model\n",
    "#y_testM = test_set.Model\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b80d4",
   "metadata": {},
   "source": [
    "## For the Confusion matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee69ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS CELL IS FOR CALCULATE THE CONFUSION MATRIX ####\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "    \n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ddf61f",
   "metadata": {},
   "source": [
    "## Import utilities from scikitlearn, models, evaluation metrics, Random Forest classifier is the model and the evaluation metric is accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91213e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b75da",
   "metadata": {},
   "source": [
    "## Random forest with sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75046e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100, max_depth=8, n_jobs=-1,\n",
    "                          criterion = 'entropy', class_weight='balanced')\n",
    "\n",
    "## n_jobs = -1 tells my computer to use all its cores, I have only 2, so it runs on 2 cores\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "rf.fit(X_train,y_trainl)\n",
    "\n",
    "ypred = rf.predict(X_test)\n",
    "#####\n",
    "\n",
    "print('Traning and Testing on raw data, all features \\n');\n",
    "#### Model accuracy\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_testl, ypred))\n",
    "\n",
    "for i, score_forest in enumerate(cross_val_score(rf, X, ylabels, cv = 3)):\n",
    "    print('Random Forest accuracy for the %d score: %0.2f' % (i, score_forest))\n",
    "score_forest=cross_val_score(rf, X, ylabels, cv=3)\n",
    "#score_tree\n",
    "cv_scores = []\n",
    "print(\"Random Forest Accuracy: %0.2f (+/- %0.2f)\" % (score_forest.mean(), score_forest.std() * 2 ))\n",
    "cv_score = score_forest.mean()\n",
    "cv_scores.append(cv_score)\n",
    "\n",
    "### We perform a cross validation score to see how accurate our decision tree is by \n",
    "### splitting the dataset into 10 folds for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5ab21",
   "metadata": {},
   "source": [
    "## Perceptron with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f21163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "#para scikit-learn: (samples,features)\n",
    "#test_input_T = test_input.T\n",
    "#X_train, X_test, y_train, y_test = train_test_split(test_input_T,test_labels,test_size=0.25,random_state=None)\n",
    "\n",
    "#ANN\n",
    "test1_model = models.Sequential()\n",
    "test1_model.add(layers.Dense(100,activation='relu',input_shape=(10,)))\n",
    "test1_model.add(layers.Dense(100,activation='relu'))\n",
    "#test1_model.add(layers.Dense(32,activation='relu'))\n",
    "#test1_model.add(layers.Dense(10,activation='relu'))\n",
    "test1_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "test1_model.compile(optimizer=optimizers.Adam(lr=0.0001),#RMSprop(lr=0.001),\n",
    "                    loss=losses.binary_crossentropy,\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "test1_model.summary()\n",
    "\n",
    "test1_model_history = test1_model.fit(X_train,\n",
    "                                      y_train,\n",
    "                                      epochs=50,\n",
    "                                      batch_size=32,\n",
    "                                      validation_data=(X_test,y_test),\n",
    "                                      verbose=1)\n",
    "\n",
    "#plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(11,4.5))    \n",
    "\n",
    "results_dict = test1_model_history.history\n",
    "epochs = range(1,len(results_dict['accuracy'])+1)\n",
    "\n",
    "#accuracy\n",
    "acc_values = results_dict['accuracy']\n",
    "val_acc_values = results_dict['val_accuracy']\n",
    "    \n",
    "axs[0].plot(epochs,acc_values,color='black',label='train',linewidth=1.5,linestyle='-')\n",
    "axs[0].plot(epochs,val_acc_values,color='red',label='test',linewidth=1.5,linestyle='-')\n",
    "axs[0].set_xlabel('epochs')\n",
    "minorLocatorX = AutoMinorLocator()\n",
    "axs[0].xaxis.set_minor_locator(minorLocatorX)\n",
    "axs[0].set_ylabel('accuracy')\n",
    "axs[0].set_ylim([0.5,1.02])   \n",
    "minorLocatorY = AutoMinorLocator()\n",
    "axs[0].yaxis.set_minor_locator(minorLocatorY)\n",
    "axs[0].tick_params(which='major', length=6)\n",
    "axs[0].tick_params(which='minor', length=3, color='black')       \n",
    "axs[0].legend(loc='lower right')\n",
    "    \n",
    "#loss\n",
    "loss_values = results_dict['loss']\n",
    "val_loss_values = results_dict['val_loss']\n",
    "       \n",
    "axs[1].plot(epochs,loss_values,color='black',label='train',linewidth=1.5,linestyle='-')\n",
    "axs[1].plot(epochs,val_loss_values,color='red',label='test',linewidth=1.5,linestyle='-')\n",
    "axs[1].set_xlabel('epochs')\n",
    "minorLocatorX = AutoMinorLocator()\n",
    "axs[1].xaxis.set_minor_locator(minorLocatorX)   \n",
    "axs[1].set_ylabel('loss')\n",
    "minorLocatorY = AutoMinorLocator()\n",
    "axs[1].yaxis.set_minor_locator(minorLocatorY)\n",
    "axs[1].tick_params(which='major', length=6)\n",
    "axs[1].tick_params(which='minor', length=3, color='black')    \n",
    "axs[1].legend(loc='upper right')\n",
    "\n",
    "axs[0].set_title('Training and test accuracy after {} epochs: \\n {:.3f} (train), {:.3f} (validation)'\n",
    "                 .format(len(epochs),acc_values[-1],val_acc_values[-1]));\n",
    "\n",
    "axs[1].set_title('Training and test loss after {} epochs: \\n {:.3f} (train), {:.3f} (validation)'\n",
    "                 .format(len(epochs),loss_values[-1],val_loss_values[-1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cb9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
